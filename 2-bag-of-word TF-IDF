from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk
import numpy as np

nltk.download('punkt')

corpus = ["i love cats", "cats are cute", "dogs are loyal", "i love dogs"]

vectorizer = CountVectorizer()
BOW = vectorizer.fit_transform(corpus)
print("number of Words\n:", BOW.toarray())
print("\nwords:", vectorizer.get_feature_names_out())

tf = BOW.toarray().astype(float)
tf = tf/tf.sum(axis=1, keepdims= True)
print("Normalized Count:", tf)

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(corpus)
print("TFIDF:", tfidf_matrix.toarray())
print("words:", tfidf.get_feature_names_out())

tokenized = [word_tokenize(sentence) for sentence in corpus]
model = Word2Vec(tokenized, vector_size= 50, window = 2, min_count = 1)

print("word2vec vector for cats:", model.wv['cats'])
      
