from collections import Counter
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

# Sample text
text = ['the quick brown fox', 'the slow brown dog', 'the quick red dog', 'the lazy yellow fox']

# Tokenize the text
tokens = [word_tokenize(sentence) for sentence in text]

# Flatten the list of tokenized sentences into a single list of words
flattened_tokens = [word for sublist in tokens for word in sublist]

# Define a function for n-gram generation
def generate_ngrams(tokens, n):
    return list(ngrams(tokens, n))

# Example: Generate bigrams (2-grams)
bigrams = generate_ngrams(flattened_tokens, 2)

# Display bigrams
print("Bigrams:", bigrams)

# Display frequency count of bigrams
bigram_freq = Counter(bigrams)
print("Bigram Frequency:", bigram_freq)
