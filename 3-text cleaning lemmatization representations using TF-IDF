import nltk
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Sample data
texts = [
    "The cats are running in the garden!",
    "Dogs barked loudly, chasing the cats.",
    "Birds are flying high in the sky.",
    "I love my pet dog."
]
labels = ['animal', 'animal', 'animal', 'emotion']

# Basic Cleaning
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

cleaned = []
for sentence in texts:
    sentence = sentence.lower()
    tokens = nltk.word_tokenize(sentence)
    tokens = [w for w in tokens if w.isalpha() and w not in stop_words]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]
    cleaned.append(" ".join(tokens))

# Label Encoding
le = LabelEncoder()
encoded_labels = le.fit_transform(labels)

# TF-IDF
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(cleaned)

# Create DataFrame and save
df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())
df['label'] = encoded_labels
df.to_csv("easy_output.csv", index=False)

print("âœ… Saved as easy_output.csv")
